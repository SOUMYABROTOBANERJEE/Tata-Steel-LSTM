{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(df1,date):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import preprocessing\n",
    "    #df1=df1[df1['Status']=='Overdue']\n",
    "    #print(df1['Status'])\n",
    "    df1.dropna(inplace=True)\n",
    "    df1 = df1.reset_index()\n",
    "    df1.drop(columns='index',inplace=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    df1['Pstng.Date']=pd.to_datetime(df1['Pstng.Date'],dayfirst=True).dt.strftime(\"%Y%m%d\").astype(int)\n",
    "    df1['Net.Due.Dt']=pd.to_datetime(df1['Net.Due.Dt'],dayfirst=True).dt.strftime(\"%Y%m%d\").astype(int)\n",
    "    df1['Clearing']=pd.to_datetime(df1['Clearing']).dt.strftime(\"%Y%m%d\").astype(int)   \n",
    "    df=df1[df1['Pstng.Date'] < date ]\n",
    "    df3=df[df['Clearing'] > date] \n",
    "    df.drop(df[df['Clearing'] > date].index,inplace=True)\n",
    "    df1=df\n",
    "    df1 = df1.reset_index()\n",
    "    df1.drop(columns='index',inplace=True)\n",
    "    \n",
    "    \n",
    "    df3 = df3.reset_index()\n",
    "    df3.drop(columns='index',inplace=True)\n",
    "    \n",
    "    \n",
    "    df2=df1[['Pstng.Date','Net.Due.Dt','Clearing','Status']]\n",
    "    df2.dropna(inplace=True)\n",
    "    df2['Range of Delay']=pd.to_datetime(df2['Clearing'],format='%Y%m%d') - pd.to_datetime(df2['Net.Due.Dt'],format='%Y%m%d')\n",
    "    df2['PayT']= pd.to_datetime(df2['Net.Due.Dt'],format='%Y%m%d') - pd.to_datetime(df2['Pstng.Date'],format='%Y%m%d')\n",
    "    df2['Range of Delay']=df2['Range of Delay'].dt.days\n",
    "    df2['Range of Delay']=pd.to_numeric(df2['Range of Delay'])\n",
    "    df2['PayT']=df2['PayT'].dt.days\n",
    "    df2['PayT']=pd.to_numeric(df2['PayT'])\n",
    "    \n",
    "    df2 = df2.reset_index()\n",
    "    df2.drop(columns = 'index',inplace=True)\n",
    "    \n",
    "    df2['Status'] = np.where(df2['Range of Delay'] > 7, 2, (np.where(df2['Range of Delay'] > 0, 1,0)))#(np.where(df2['Range of Delay'] > 0, 1,0)))))#(np.where(df2['Range of Delay'] > 0, 1,0)))))#(np.where(df2['Range of Delay'] > 10, 2,(np.where(df2['Range of Delay'] > 0,1,0)))))))# (np.where(df2['Range of Delay'] > 30, 5, (np.where(df2['Range of Delay'] > 15, 4, (np.where(df2['Range of Delay'] > 7, 3, (np.where(df2['Range of Delay'] > 3, 2, (np.where(df2['Range of Delay'] > 0, 1 ,0)))))))))))))))\n",
    "    #print(df2)\n",
    "    print('Y s in the initial set')\n",
    "    print(np.unique(df2['Status'],return_counts=True))\n",
    "    print('----------------------------------------------------------------------------------------')\n",
    "    \n",
    "    df1['Status']=df2['Status']\n",
    "    df1['PayT']=df2['PayT']\n",
    "    df1.dropna(inplace=True)\n",
    "    y=df1['Status']\n",
    "    \n",
    "    \n",
    "    busa = preprocessing.LabelEncoder()\n",
    "    ccar= preprocessing.LabelEncoder()\n",
    "    month=preprocessing.LabelEncoder()\n",
    "    zone=preprocessing.LabelEncoder()\n",
    "    bran=preprocessing.LabelEncoder()\n",
    "    payt=preprocessing.LabelEncoder()\n",
    "    \n",
    "    df1.drop(columns='Status',inplace=True)\n",
    "    #df1['Status']=df1['Status'].astype('category').cat.codes\n",
    "    arr=busa.fit(df1['BusA'])\n",
    "    df1['BusA']=busa.transform(df1['BusA'])\n",
    "    arr=ccar.fit(df1['CCAr'])\n",
    "    df1['CCAr']=ccar.transform(df1['CCAr'])#.astype('category').cat.codes\n",
    "    #df1['Account']=df1['Account'].astype('category').cat.codes\n",
    "    arr=month.fit(df1['Month'])\n",
    "    df1['Month']=month.transform(df1['Month'])#.astype('category').cat.codes\n",
    "    df1.drop(columns='Reference',inplace=True)\n",
    "    df1.drop(columns='Customer.Name',inplace=True)\n",
    "    df1.drop(columns=['DocumentNo','Year','Clrng.doc.'],inplace=True)\n",
    "    arr=zone.fit(df1['Zone'])\n",
    "    df1['Zone']=zone.transform(df1['Zone'])    #.astype('category').cat.codes\n",
    "    arr=bran.fit(df1['Bran'])\n",
    "    df1['Bran']=bran.transform(df1['Bran'])#.astype('category').cat.codes\n",
    "    arr =payt.fit(df1['PayT'])\n",
    "    #df1['PayT'] = payt.transform(df1['PayT'])#.astype('category').cat.codes\n",
    "    df1.drop(columns='Doc.Chq.dt',inplace=True)\n",
    "    df1.drop(columns='Ty',inplace=True)\n",
    "    df1.drop(columns='Sale.Type',inplace=True)\n",
    "    \n",
    "    \n",
    "    #df1.drop(columns='Clearing',inplace=True)\n",
    "    df1 = df1.reset_index()\n",
    "    df1.drop(columns='index',inplace=True)\n",
    "    df1.drop(columns='Arr..Clearing...Net.Due.Date.',inplace=True)\n",
    "    df1.drop(columns='G.L',inplace=True)\n",
    "    df1.drop(columns='Clearing',inplace=True)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    #print('Creating Red Alert Zone')\n",
    "    #table = pd.crosstab(columns=y, index=df1['Account'])\n",
    "    #table.columns= [\"No delay\",\" 0 - 10\", \">10\"]\n",
    "    #print(table)\n",
    "    #print(table.sort_values(['>10'],ascending=False).head())\n",
    "    #arr=table.sort_values(['>10'],ascending=False)\n",
    "    #arr.drop(columns=[\"No delay\",\" 0 - 10\"],inplace=True)\n",
    "    #df_1=arr[arr['>10']>40]\n",
    "    #df_2=[]\n",
    "    \n",
    "    \n",
    "    #df1['Account']=df1['Account']/100000\n",
    "    #df1['Reference']=df1['Reference']/1000000000\n",
    "    #df1['Pstng.Date']=df1['Pstng.Date']/100000000\n",
    "    #df1['Net.Due.Dt']=df1['Pstng.Date']/100000000\n",
    "    \n",
    "\n",
    "\n",
    "    #print(df_1.index)\n",
    "    \n",
    "    \n",
    "    ###################Oversampling##############################\n",
    "    from imblearn.over_sampling import ADASYN \n",
    "    sm = ADASYN()\n",
    "    df4, y = sm.fit_resample(df1, y)\n",
    "    df1=pd.DataFrame(df4,columns=df1.columns)\n",
    "    ############################################################\n",
    "    \n",
    "    #y1=df1.columns\n",
    "    #x = df1.values #returns a numpy array\n",
    "    #min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    #x_scaled = min_max_scaler.fit_transform(x)\n",
    "    #df1 = pd.DataFrame(x_scaled,columns=df1.columns)\n",
    "    \n",
    "    print('Y s in the set')\n",
    "    print(np.unique(y,return_counts=True))\n",
    "    print('----------------------------------------------------------------------------------------')\n",
    "    \n",
    "    return df1,df3,y,busa,ccar,month,zone,bran,payt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(df1,y):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import metrics\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn import tree\n",
    "    from sklearn import neighbors\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.ensemble import VotingClassifier\n",
    "    from sklearn.ensemble import BaggingClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers.recurrent import LSTM\n",
    "    from keras.layers.core import Dense, Activation, Dropout\n",
    "    #y=np.array(df1['Status'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    X=df1\n",
    "    #df1.head(5)\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size = 0.3, random_state=0)\n",
    "    # Random forest Model: \n",
    "    model= RandomForestClassifier(n_estimators = 300,random_state =30,n_jobs=3,oob_score=True,verbose=2,max_features=None,class_weight={0:100,1:10000,2:500})\n",
    "    # Decion Tree Model: \n",
    "    decision_tree = tree.DecisionTreeClassifier(random_state=0, max_depth=100)\n",
    "    # AdaBoost Classifier Model:\n",
    "    clf = AdaBoostClassifier(base_estimator=model,n_estimators=100, random_state=0)\n",
    "    # KNN :\n",
    "    mo = neighbors.KNeighborsClassifier(n_neighbors =2)\n",
    "    # Bagging Classifier Model:  \n",
    "    bagging = BaggingClassifier(mo,max_samples=0.5, max_features=0.5)\n",
    "    #Voting Classifier\n",
    "    eclf = model#VotingClassifier(estimators=[ ('knn_bagging', bagging), ('adaboost', clf), ('RF',model)],voting='soft')\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(4, input_shape = ))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss = \"mean_squared_error\",  optimizer = \"adam\")\n",
    "    model.fit(xtrain, ytrain, epochs = 100, batch_size = 1, verbose = 2)\n",
    "    \n",
    "    \n",
    "    print('TRAINING DONE ......................')\n",
    "    eclf1=model.predict(xtrain)\n",
    "    eclf2=model.predict(xtest)\n",
    "    print('TRAINING RESULTS-----------------------------------------')\n",
    "    print(\"Accuracy: for Training :\",metrics.accuracy_score(ytrain, eclf1)*100)\n",
    "    print(\"Accuracy: for Testing :\",metrics.accuracy_score(ytest, eclf2)*100)\n",
    "    print()\n",
    "    print('Confusion Matrix for Random Forest')\n",
    "    print(metrics.confusion_matrix(eclf2, ytest))\n",
    "    print()\n",
    "    print('--------------------------------------------------------------------------------------------------------------------')\n",
    "    return eclf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df1,clf,busa,ccar,month,zone,bran,payt):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import copy\n",
    "    from sklearn import preprocessing\n",
    "    df=copy.deepcopy(df1)\n",
    "    print(df.shape)\n",
    "    df['Prediction']=np.zeros(len(df1))\n",
    "    print(df1.shape)\n",
    "    df1.dropna(inplace=True)\n",
    "    df1 = df1.reset_index()\n",
    "    df1.drop(columns='index',inplace=True)\n",
    "    #df1['Status']=df1['Status'].astype('category').cat.codes)\n",
    "    #df1['Status']=df1['Status'].astype('category').cat.codes\n",
    "    \n",
    "    df1['PayT']= pd.to_datetime(df1['Net.Due.Dt'],format='%Y%m%d') - pd.to_datetime(df1['Pstng.Date'],format='%Y%m%d')\n",
    "    df1['PayT']=df1['PayT'].dt.days\n",
    "    df1['PayT']=pd.to_numeric(df1['PayT'])\n",
    "    df1.drop(columns='Clearing',inplace=True)\n",
    "    df1 = df1.reset_index()\n",
    "    df1.drop(columns = 'index',inplace=True)\n",
    "    #df1['Status']=df1['Status'].astype('category').cat.codes\n",
    "    df1['BusA']=busa.transform(df1['BusA'])\n",
    "    df1['CCAr']=ccar.transform(df1['CCAr'])#.astype('category').cat.codes\n",
    "    #df1['Account']=df1['Account'].astype('category').cat.codes\n",
    "    df1['Month']=month.transform(df1['Month'])#.astype('category').cat.codes\n",
    "    df1.drop(columns='Reference',inplace=True)\n",
    "    df1.drop(columns='Customer.Name',inplace=True)\n",
    "    df1.drop(columns=['DocumentNo','Year'],inplace=True)\n",
    "    df1['Zone']=zone.transform(df1['Zone'])#.astype('category').cat.codes\n",
    "    df1['Bran']=bran.transform(df1['Bran'])#.astype('category').cat.codes\n",
    "    #df1['PayT'] =payt.transform(df1['PayT'])#.astype('category').cat.codes\n",
    "    df1.drop(columns='Doc.Chq.dt',inplace=True)\n",
    "    df1.drop(columns='Ty',inplace=True)\n",
    "    df1.drop(columns='Sale.Type',inplace=True)\n",
    "    #df1['Pstng.Date']=pd.to_datetime(df1['Pstng.Date'],dayfirst=True).dt.strftime(\"%Y%m%d\").astype(int)\n",
    "    #df1['Net.Due.Dt']=pd.to_datetime(df1['Net.Due.Dt'],dayfirst=True).dt.strftime(\"%Y%m%d\").astype(int)\n",
    "    #df1['Clearing']=pd.to_datetime(df1['Clearing']).dt.strftime(\"%Y%m%d\").astype(str)\n",
    "    #df1.drop(columns=['Net.Due.Dt','Clearing'],inplace=True)\n",
    "    #df1.drop(columns='Net.Due.Dt',inplace=True)\n",
    "    df1 = df1.reset_index()\n",
    "    df1.drop(columns='index',inplace=True)\n",
    "    df1.drop(columns='Arr..Clearing...Net.Due.Date.',inplace=True)\n",
    "    df1.drop(columns='G.L',inplace=True)\n",
    "    \n",
    "    #y1=df1.columns\n",
    "    #x = df1.values #returns a numpy array\n",
    "    #min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    #x_scaled = min_max_scaler.fit_transform(x)\n",
    "    #df1 = pd.DataFrame(x_scaled,columns=df1.columns)\n",
    "    \n",
    "    #df1['Account']=df1['Account']/100000\n",
    "    #df1['Reference']=df1['Reference']/1000000000\n",
    "    #df1['Pstng.Date']=df1['Pstng.Date']/100000000\n",
    "    #df1['Net.Due.Dt']=df1['Pstng.Date']/100000000\n",
    "    \n",
    "    \n",
    "    print('Starting Prediction ....')\n",
    "    prediction=clf.predict(df1)\n",
    "    print('Generating Probabilities.....')\n",
    "    predictprob=clf.predict_proba(df1)\n",
    "    print('Finished prediction...')\n",
    "    print()\n",
    "    #df['Probability']=np.zeros(len(df))\n",
    "    print(df1.head())\n",
    "    \n",
    "\n",
    "    arr=['No Delay','Low Delay','High Delay']\n",
    "    arr1=['Low delay','Low Medium Delay','High Delay']#,'High Delay']#,'30 to 180','>180']#,'8-15 days','16-30 days','31-60 days','61-90 days','90-180 days','>180 days']\n",
    "    for i in range(len(arr)):\n",
    "        df[arr1[i]]=np.empty(len(df))\n",
    "    print('Computing Prediction Table............')\n",
    "    #for i in range(len(prediction)):\n",
    "        #flag=0\n",
    "        #if ( i % 500 == 0):\n",
    "         #   print('----------------------------------------------------------------------------------------------------------- i =',i)\n",
    "        #for k in range(len(redzone)):\n",
    "           # if df['Account'][i] == redzone[k] :\n",
    "             #   df['Prediction'][i]='>=10'\n",
    "              #  flag=1\n",
    "              #  print('----------------------------------------------------------------RED ALERT------------------------------------------ ')\n",
    "                \n",
    "        #if(flag == 0):\n",
    "    \n",
    "        #print(arr[prediction[i]])\n",
    "        #arr2=\"\"\n",
    "        #print('Actual:',df['Arr..Clearing...Net.Due.Date.'][i])\n",
    "        #print('--------------------------------------------------------')\n",
    "    df['Prediction']=prediction\n",
    "    #df['Prediction']=np.where(df['Prediction'] == 0, 'No Delay',(np.where(df['Prediction'] == 1, 'Low Delay',(np.where(df['Prediction'] == 2, 'High Delay','')))))\n",
    "    for i in range(len(arr1)):\n",
    "        df[arr1[i]]=np.around(predictprob[:,i],decimals=2)\n",
    "        \n",
    "    print(df.head(5))\n",
    "    #df['Payment Within Next Few Days']=np.where(df['Prediction'] == 1, 'Yes',(np.where((df['Low delay']>0.2) & (df['High Delay']<0.4)),'Yes','No'))\n",
    "    print()\n",
    "    print(df.head())\n",
    "    print('Writing to csv file')\n",
    "    print('-----------------------------------------------------------------------------------------')\n",
    "    df.to_csv('Predictedout6.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Finished Reading...\n",
      "\n",
      "   BusA CCAr     Reference  Account          Customer.Name   Zone  Bran  Ty  \\\n",
      "0  9600  BPR  1.115502e+09   928860  TATA STEEL PROCESSING  North  GBAD  RV   \n",
      "1  9600  BPR  1.115502e+09   928860  TATA STEEL PROCESSING  North  GBAD  RV   \n",
      "2  2500  BPR  2.410320e+09   928860  TATA STEEL PROCESSING  North  GBAD  RV   \n",
      "3  1000  BPR  2.412144e+09   928860  TATA STEEL PROCESSING  North  GBAD  RV   \n",
      "4  1000  BPR  2.412161e+09   928860  TATA STEEL PROCESSING  North  GBAD  RV   \n",
      "\n",
      "    Status  Local.Crcy.Amt  ...  Doc.Chq.dt Month Pstng.Date  Net.Due.Dt  \\\n",
      "0  Overdue            0.04  ...  31.10.2018   Oct   31-10-18    10-11-18   \n",
      "1   Not OD            0.12  ...  05.11.2018   Nov   05-11-18    15-11-18   \n",
      "2  Overdue            0.16  ...  03.11.2018   Nov   03-11-18    13-11-18   \n",
      "3  Overdue            0.14  ...  31.10.2018   Oct   31-10-18    10-11-18   \n",
      "4   Not OD            0.14  ...  16.12.2018   Dec   16-12-18    26-12-18   \n",
      "\n",
      "       G.L  Year Clrng.doc.  Clearing  Pending_Amy  pending_cnt  \n",
      "0  1221001  2019   93892774  15-11-18         0.60            4  \n",
      "1  1221001  2019   93892774  15-11-18         1.42           10  \n",
      "2  1221001  2019   93892774  15-11-18         1.26            9  \n",
      "3  1221001  2019   93892774  15-11-18         0.60            4  \n",
      "4  1221001  2019   94154498  26-12-18         0.00            0  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "\n",
      "Preprocessing started .......\n",
      "Y s in the initial set\n",
      "(array([0, 1, 2]), array([70220, 12017, 11414], dtype=int64))\n",
      "----------------------------------------------------------------------------------------\n",
      "Y s in the set\n",
      "(array([0, 1, 2]), array([70220, 71167, 70448], dtype=int64))\n",
      "----------------------------------------------------------------------------------------\n",
      "   BusA  CCAr   Account  Zone  Bran  Local.Crcy.Amt  PayT  Month  Pstng.Date  \\\n",
      "0   6.0   0.0  928860.0   1.0   9.0            0.04  10.0    6.0  20181031.0   \n",
      "1   6.0   0.0  928860.0   1.0   9.0            0.12  10.0    5.0  20181105.0   \n",
      "2   2.0   0.0  928860.0   1.0   9.0            0.16  10.0    5.0  20181103.0   \n",
      "3   0.0   0.0  928860.0   1.0   9.0            0.14  10.0    6.0  20181031.0   \n",
      "4   6.0   0.0  928860.0   1.0   9.0            0.16   5.0    5.0  20181105.0   \n",
      "\n",
      "   Net.Due.Dt  Pending_Amy  pending_cnt  \n",
      "0  20181110.0         0.60          4.0  \n",
      "1  20181115.0         1.42         10.0  \n",
      "2  20181113.0         1.26          9.0  \n",
      "3  20181110.0         0.60          4.0  \n",
      "4  20181110.0         1.42         10.0  \n",
      "Preprocessing Done...........\n",
      "--------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "trainpath='My Data_Delay.csv'\n",
    "\n",
    "print('Reading data...')\n",
    "df1=pd.read_csv(trainpath)\n",
    "print('Finished Reading...')\n",
    "print()\n",
    "print(df1.head(5))\n",
    "print()\n",
    "date=20181201\n",
    "df3=copy.deepcopy(df1)\n",
    "print('Preprocessing started .......')\n",
    "df2,df4,y,busa,ccar,month,zone,bran,payt=preprocessor(df1,date)\n",
    "print(df2.head(5))\n",
    "print('Preprocessing Done...........')\n",
    "print('--------------------------------------------------------------------------------------------------------')\n",
    "#print('RED ZONE VISUALISATION....')\n",
    "\n",
    "#visualisation_red(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.drop(columns=['Status','Clrng.doc.'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Starting............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Laptop\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected lstm_1_input to have 3 dimensions, but got array with shape (148284, 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-5fb0f9018f94>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mt1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training Starting............'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training Done...................'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Time Taken'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-d1e9487d7e29>\u001b[0m in \u001b[0;36mtrainer\u001b[1;34m(df1, y)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"mean_squared_error\"\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"adam\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    126\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    129\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected lstm_1_input to have 3 dimensions, but got array with shape (148284, 12)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1=time.time()\n",
    "print('Training Starting............')\n",
    "clf=trainer(df2,y)\n",
    "print('Training Done...................')\n",
    "print('Time Taken',time.time()-t1)\n",
    "print('-----------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances\n",
      "('BusA', 0.05102617907456457)\n",
      "('CCAr', 0.0)\n",
      "('Account', 0.1549349880830567)\n",
      "('Zone', 0.03731605800307801)\n",
      "('Bran', 0.0860613122837913)\n",
      "('Local.Crcy.Amt', 0.10598695992884784)\n",
      "('PayT', 0.06240314414088058)\n",
      "('Month', 0.06473480204750053)\n",
      "('Pstng.Date', 0.09680186211229673)\n",
      "('Net.Due.Dt', 0.17937572838534319)\n",
      "('Pending_Amy', 0.08722421231181372)\n",
      "('pending_cnt', 0.07413475362882688)\n"
     ]
    }
   ],
   "source": [
    "print('Feature Importances')\n",
    "for feature in zip(df2.columns, clf.feature_importances_):\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1479, 22)\n",
      "(1479, 22)\n",
      "Starting Prediction ....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  35 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 156 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 300 out of 300 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  35 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 156 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Probabilities.....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done 300 out of 300 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished prediction...\n",
      "\n",
      "   BusA  CCAr  Account  Zone  Bran  Local.Crcy.Amt  PayT  Month  Pstng.Date  \\\n",
      "0     0     0   928860     1     9            0.14    10      2    20181216   \n",
      "1     0     0   928860     1     9            0.14    10      2    20181217   \n",
      "2     6     0   927881     2    21            0.18    10      2    20181214   \n",
      "3     6     0   927881     2    21            0.18     5      2    20181214   \n",
      "4     6     0   927881     2    21            0.18     5      2    20181214   \n",
      "\n",
      "   Net.Due.Dt  Pending_Amy  pending_cnt  \n",
      "0    20181226         0.00            0  \n",
      "1    20181227         0.14            1  \n",
      "2    20181224         2.16           12  \n",
      "3    20181219         2.16           12  \n",
      "4    20181219         2.16           12  \n",
      "Computing Prediction Table............\n",
      "   BusA CCAr     Reference  Account          Customer.Name   Zone  Bran  Ty  \\\n",
      "0  1000  BPR  2.412161e+09   928860  TATA STEEL PROCESSING  North  GBAD  RV   \n",
      "1  1000  BPR  2.412161e+09   928860  TATA STEEL PROCESSING  North  GBAD  RV   \n",
      "2  9600  BPR  2.147091e+09   927881   SAMRAT IRONS PVT LTD  South  SBAD  RV   \n",
      "3  9600  BPR  2.147091e+09   927881   SAMRAT IRONS PVT LTD  South  SBAD  RV   \n",
      "4  9600  BPR  2.147091e+09   927881   SAMRAT IRONS PVT LTD  South  SBAD  RV   \n",
      "\n",
      "   Local.Crcy.Amt  Arr..Clearing...Net.Due.Date.  ... Net.Due.Dt      G.L  \\\n",
      "0            0.14                              0  ...   20181226  1221001   \n",
      "1            0.14                             -1  ...   20181227  1221001   \n",
      "2            0.18                              5  ...   20181224  1221001   \n",
      "3            0.18                              2  ...   20181219  1221001   \n",
      "4            0.18                              2  ...   20181219  1221001   \n",
      "\n",
      "   Year  Clearing Pending_Amy  pending_cnt  Prediction  Low delay  \\\n",
      "0  2019  20181226        0.00            0           2       0.23   \n",
      "1  2019  20181226        0.14            1           0       1.00   \n",
      "2  2019  20181229        2.16           12           0       0.86   \n",
      "3  2019  20181221        2.16           12           1       0.15   \n",
      "4  2019  20181221        2.16           12           1       0.15   \n",
      "\n",
      "   Low Medium Delay  High Delay  \n",
      "0              0.08        0.69  \n",
      "1              0.00        0.00  \n",
      "2              0.14        0.00  \n",
      "3              0.83        0.01  \n",
      "4              0.83        0.01  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "\n",
      "   BusA CCAr     Reference  Account          Customer.Name   Zone  Bran  Ty  \\\n",
      "0  1000  BPR  2.412161e+09   928860  TATA STEEL PROCESSING  North  GBAD  RV   \n",
      "1  1000  BPR  2.412161e+09   928860  TATA STEEL PROCESSING  North  GBAD  RV   \n",
      "2  9600  BPR  2.147091e+09   927881   SAMRAT IRONS PVT LTD  South  SBAD  RV   \n",
      "3  9600  BPR  2.147091e+09   927881   SAMRAT IRONS PVT LTD  South  SBAD  RV   \n",
      "4  9600  BPR  2.147091e+09   927881   SAMRAT IRONS PVT LTD  South  SBAD  RV   \n",
      "\n",
      "   Local.Crcy.Amt  Arr..Clearing...Net.Due.Date.  ... Net.Due.Dt      G.L  \\\n",
      "0            0.14                              0  ...   20181226  1221001   \n",
      "1            0.14                             -1  ...   20181227  1221001   \n",
      "2            0.18                              5  ...   20181224  1221001   \n",
      "3            0.18                              2  ...   20181219  1221001   \n",
      "4            0.18                              2  ...   20181219  1221001   \n",
      "\n",
      "   Year  Clearing Pending_Amy  pending_cnt  Prediction  Low delay  \\\n",
      "0  2019  20181226        0.00            0           2       0.23   \n",
      "1  2019  20181226        0.14            1           0       1.00   \n",
      "2  2019  20181229        2.16           12           0       0.86   \n",
      "3  2019  20181221        2.16           12           1       0.15   \n",
      "4  2019  20181221        2.16           12           1       0.15   \n",
      "\n",
      "   Low Medium Delay  High Delay  \n",
      "0              0.08        0.69  \n",
      "1              0.00        0.00  \n",
      "2              0.14        0.00  \n",
      "3              0.83        0.01  \n",
      "4              0.83        0.01  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "Writing to csv file\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "______________________________________  END  ____________________________________\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testpath= ''\n",
    "#test=pd.read_csv(testpath)\n",
    "predict(df4,clf,busa,ccar,month,zone,bran,payt)\n",
    "print() \n",
    "print('______________________________________  END  ____________________________________')\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1455, 1459], dtype=int64),)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(df4['PayT']=='Z020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.drop([1456],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6], dtype=int64)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "busa.transform([9600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset css and font defaults in:\n",
      "C:\\Users\\Laptop\\.jupyter\\custom &\n",
      "C:\\Users\\Laptop\\AppData\\Roaming\\jupyter\\nbextensions\n"
     ]
    }
   ],
   "source": [
    "!jt -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
