{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(df1,date):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import preprocessing\n",
    "    #df1=df1[df1['Status']=='Overdue']\n",
    "    #print(df1['Status'])\n",
    "    df1.dropna(inplace=True)\n",
    "    df1 = df1.reset_index()\n",
    "    df1.drop(columns='index',inplace=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    df1['Pstng.Date']=pd.to_datetime(df1['Pstng.Date'],dayfirst=True).dt.strftime(\"%Y%m%d\").astype(int)\n",
    "    df1['Net.Due.Dt']=pd.to_datetime(df1['Net.Due.Dt'],dayfirst=True).dt.strftime(\"%Y%m%d\").astype(int)\n",
    "    df1['Clearing']=pd.to_datetime(df1['Clearing']).dt.strftime(\"%Y%m%d\").astype(int)   \n",
    "    df=df1[df1['Pstng.Date'] < date ]\n",
    "    df3=df[df['Clearing'] > date] \n",
    "    df.drop(df[df['Clearing'] > date].index,inplace=True)\n",
    "    df1=df\n",
    "    df1 = df1.reset_index()\n",
    "    df1.drop(columns='index',inplace=True)\n",
    "    \n",
    "    \n",
    "    df3 = df3.reset_index()\n",
    "    df3.drop(columns='index',inplace=True)\n",
    "    \n",
    "    \n",
    "    df2=df1[['Pstng.Date','Net.Due.Dt','Clearing','Status']]\n",
    "    df2.dropna(inplace=True)\n",
    "    df2['Range of Delay']=pd.to_datetime(df2['Clearing'],format='%Y%m%d') - pd.to_datetime(df2['Net.Due.Dt'],format='%Y%m%d')\n",
    "    df2['PayT']= pd.to_datetime(df2['Net.Due.Dt'],format='%Y%m%d') - pd.to_datetime(df2['Pstng.Date'],format='%Y%m%d')\n",
    "    df2['Range of Delay']=df2['Range of Delay'].dt.days\n",
    "    df2['Range of Delay']=pd.to_numeric(df2['Range of Delay'])\n",
    "    df2['PayT']=df2['PayT'].dt.days\n",
    "    df2['PayT']=pd.to_numeric(df2['PayT'])\n",
    "    \n",
    "    df2 = df2.reset_index()\n",
    "    df2.drop(columns = 'index',inplace=True)\n",
    "    \n",
    "    df2['Status'] = np.where(df2['Range of Delay'] > 7, 2, (np.where(df2['Range of Delay'] > 0, 1,0)))#(np.where(df2['Range of Delay'] > 0, 1,0)))))#(np.where(df2['Range of Delay'] > 0, 1,0)))))#(np.where(df2['Range of Delay'] > 10, 2,(np.where(df2['Range of Delay'] > 0,1,0)))))))# (np.where(df2['Range of Delay'] > 30, 5, (np.where(df2['Range of Delay'] > 15, 4, (np.where(df2['Range of Delay'] > 7, 3, (np.where(df2['Range of Delay'] > 3, 2, (np.where(df2['Range of Delay'] > 0, 1 ,0)))))))))))))))\n",
    "    #print(df2)\n",
    "    print('Y s in the initial set')\n",
    "    print(np.unique(df2['Status'],return_counts=True))\n",
    "    print('----------------------------------------------------------------------------------------')\n",
    "    \n",
    "    df1['Status']=df2['Status']\n",
    "    df1['PayT']=df2['PayT']\n",
    "    df1.dropna(inplace=True)\n",
    "    y=df1['Status']\n",
    "    \n",
    "    \n",
    "    busa = preprocessing.LabelEncoder()\n",
    "    ccar= preprocessing.LabelEncoder()\n",
    "    month=preprocessing.LabelEncoder()\n",
    "    zone=preprocessing.LabelEncoder()\n",
    "    bran=preprocessing.LabelEncoder()\n",
    "    payt=preprocessing.LabelEncoder()\n",
    "    \n",
    "    df1.drop(columns='Status',inplace=True)\n",
    "    #df1['Status']=df1['Status'].astype('category').cat.codes\n",
    "    arr=busa.fit(df1['BusA'])\n",
    "    df1['BusA']=busa.transform(df1['BusA'])\n",
    "    arr=ccar.fit(df1['CCAr'])\n",
    "    df1['CCAr']=ccar.transform(df1['CCAr'])#.astype('category').cat.codes\n",
    "    #df1['Account']=df1['Account'].astype('category').cat.codes\n",
    "    arr=month.fit(df1['Month'])\n",
    "    df1['Month']=month.transform(df1['Month'])#.astype('category').cat.codes\n",
    "    df1.drop(columns='Reference',inplace=True)\n",
    "    df1.drop(columns='Customer.Name',inplace=True)\n",
    "    df1.drop(columns=['DocumentNo','Year','Clrng.doc.'],inplace=True)\n",
    "    arr=zone.fit(df1['Zone'])\n",
    "    df1['Zone']=zone.transform(df1['Zone'])    #.astype('category').cat.codes\n",
    "    arr=bran.fit(df1['Bran'])\n",
    "    df1['Bran']=bran.transform(df1['Bran'])#.astype('category').cat.codes\n",
    "    arr =payt.fit(df1['PayT'])\n",
    "    #df1['PayT'] = payt.transform(df1['PayT'])#.astype('category').cat.codes\n",
    "    df1.drop(columns='Doc.Chq.dt',inplace=True)\n",
    "    df1.drop(columns='Ty',inplace=True)\n",
    "    df1.drop(columns='Sale.Type',inplace=True)\n",
    "    \n",
    "    \n",
    "    #df1.drop(columns='Clearing',inplace=True)\n",
    "    df1 = df1.reset_index()\n",
    "    df1.drop(columns='index',inplace=True)\n",
    "    df1.drop(columns='Arr..Clearing...Net.Due.Date.',inplace=True)\n",
    "    df1.drop(columns='G.L',inplace=True)\n",
    "    df1.drop(columns='Clearing',inplace=True)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    #print('Creating Red Alert Zone')\n",
    "    #table = pd.crosstab(columns=y, index=df1['Account'])\n",
    "    #table.columns= [\"No delay\",\" 0 - 10\", \">10\"]\n",
    "    #print(table)\n",
    "    #print(table.sort_values(['>10'],ascending=False).head())\n",
    "    #arr=table.sort_values(['>10'],ascending=False)\n",
    "    #arr.drop(columns=[\"No delay\",\" 0 - 10\"],inplace=True)\n",
    "    #df_1=arr[arr['>10']>40]\n",
    "    #df_2=[]\n",
    "    \n",
    "    \n",
    "    #df1['Account']=df1['Account']/100000\n",
    "    #df1['Reference']=df1['Reference']/1000000000\n",
    "    #df1['Pstng.Date']=df1['Pstng.Date']/100000000\n",
    "    #df1['Net.Due.Dt']=df1['Pstng.Date']/100000000\n",
    "    \n",
    "\n",
    "\n",
    "    #print(df_1.index)\n",
    "    \n",
    "    \n",
    "    ###################Oversampling##############################\n",
    "    from imblearn.over_sampling import ADASYN \n",
    "    sm = ADASYN()\n",
    "    df4, y = sm.fit_resample(df1, y)\n",
    "    df1=pd.DataFrame(df4,columns=df1.columns)\n",
    "    ############################################################\n",
    "    \n",
    "    #y1=df1.columns\n",
    "    #x = df1.values #returns a numpy array\n",
    "    #min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    #x_scaled = min_max_scaler.fit_transform(x)\n",
    "    #df1 = pd.DataFrame(x_scaled,columns=df1.columns)\n",
    "    \n",
    "    print('Y s in the set')\n",
    "    print(np.unique(y,return_counts=True))\n",
    "    print('----------------------------------------------------------------------------------------')\n",
    "    \n",
    "    return df1,df3,y,busa,ccar,month,zone,bran,payt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(df1,y):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import metrics\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn import tree\n",
    "    from sklearn import neighbors\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.ensemble import VotingClassifier\n",
    "    from sklearn.ensemble import BaggingClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    #y=np.array(df1['Status'])\n",
    "    \n",
    "    \n",
    "\n",
    "    X=df1\n",
    "    #df1.head(5)\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size = 0.3, random_state=0)\n",
    "    \n",
    "    \n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    \n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    y_train=enc.fit_transform(ytrain)\n",
    "    y_test=enc.fit_transform(ytest)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=13, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(6, activation='relu'))\n",
    "    model.add(Dense(6, activation='relu'))\n",
    "    model.add(Dense(4, activation='relu'))\n",
    "    model.add(Dense(2, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    # compile and fit model\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "    model.fit(xtrain, y_train, batch_size=15, epochs=2500, validation_data=(xtest, y_test))\n",
    "    \n",
    "    \n",
    "        # Random forest Model: \n",
    "    \n",
    "    print('TRAINING DONE ......................')\n",
    "    eclf1=model.predict(xtrain)\n",
    "    eclf2=model.predict(xtest)\n",
    "    print('TRAINING RESULTS-----------------------------------------')\n",
    "    print(\"Accuracy: for Training :\",metrics.accuracy_score(ytrain, eclf1)*100)\n",
    "    print(\"Accuracy: for Testing :\",metrics.accuracy_score(ytest, eclf2)*100)\n",
    "    print()\n",
    "    print('Confusion Matrix for Random Forest')\n",
    "    print(metrics.confusion_matrix(eclf2, ytest))\n",
    "    print()\n",
    "    print('--------------------------------------------------------------------------------------------------------------------')\n",
    "    return eclf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df1,clf,busa,ccar,month,zone,bran,payt):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import copy\n",
    "    from sklearn import preprocessing\n",
    "    df=copy.deepcopy(df1)\n",
    "    print(df.shape)\n",
    "    df['Prediction']=np.zeros(len(df1))\n",
    "    print(df1.shape)\n",
    "    df1.dropna(inplace=True)\n",
    "    df1 = df1.reset_index()\n",
    "    df1.drop(columns='index',inplace=True)\n",
    "    #df1['Status']=df1['Status'].astype('category').cat.codes)\n",
    "    #df1['Status']=df1['Status'].astype('category').cat.codes\n",
    "    \n",
    "    df1['PayT']= pd.to_datetime(df1['Net.Due.Dt'],format='%Y%m%d') - pd.to_datetime(df1['Pstng.Date'],format='%Y%m%d')\n",
    "    df1['PayT']=df1['PayT'].dt.days\n",
    "    df1['PayT']=pd.to_numeric(df1['PayT'])\n",
    "    df1.drop(columns='Clearing',inplace=True)\n",
    "    df1 = df1.reset_index()\n",
    "    df1.drop(columns = 'index',inplace=True)\n",
    "    #df1['Status']=df1['Status'].astype('category').cat.codes\n",
    "    df1['BusA']=busa.transform(df1['BusA'])\n",
    "    df1['CCAr']=ccar.transform(df1['CCAr'])#.astype('category').cat.codes\n",
    "    #df1['Account']=df1['Account'].astype('category').cat.codes\n",
    "    df1['Month']=month.transform(df1['Month'])#.astype('category').cat.codes\n",
    "    df1.drop(columns='Reference',inplace=True)\n",
    "    df1.drop(columns='Customer.Name',inplace=True)\n",
    "    df1.drop(columns=['DocumentNo','Year'],inplace=True)\n",
    "    df1['Zone']=zone.transform(df1['Zone'])#.astype('category').cat.codes\n",
    "    df1['Bran']=bran.transform(df1['Bran'])#.astype('category').cat.codes\n",
    "    #df1['PayT'] =payt.transform(df1['PayT'])#.astype('category').cat.codes\n",
    "    df1.drop(columns='Doc.Chq.dt',inplace=True)\n",
    "    df1.drop(columns='Ty',inplace=True)\n",
    "    df1.drop(columns='Sale.Type',inplace=True)\n",
    "    #df1['Pstng.Date']=pd.to_datetime(df1['Pstng.Date'],dayfirst=True).dt.strftime(\"%Y%m%d\").astype(int)\n",
    "    #df1['Net.Due.Dt']=pd.to_datetime(df1['Net.Due.Dt'],dayfirst=True).dt.strftime(\"%Y%m%d\").astype(int)\n",
    "    #df1['Clearing']=pd.to_datetime(df1['Clearing']).dt.strftime(\"%Y%m%d\").astype(str)\n",
    "    #df1.drop(columns=['Net.Due.Dt','Clearing'],inplace=True)\n",
    "    #df1.drop(columns='Net.Due.Dt',inplace=True)\n",
    "    df1 = df1.reset_index()\n",
    "    df1.drop(columns='index',inplace=True)\n",
    "    df1.drop(columns='Arr..Clearing...Net.Due.Date.',inplace=True)\n",
    "    df1.drop(columns='G.L',inplace=True)\n",
    "    \n",
    "    #y1=df1.columns\n",
    "    #x = df1.values #returns a numpy array\n",
    "    #min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    #x_scaled = min_max_scaler.fit_transform(x)\n",
    "    #df1 = pd.DataFrame(x_scaled,columns=df1.columns)\n",
    "    \n",
    "    #df1['Account']=df1['Account']/100000\n",
    "    #df1['Reference']=df1['Reference']/1000000000\n",
    "    #df1['Pstng.Date']=df1['Pstng.Date']/100000000\n",
    "    #df1['Net.Due.Dt']=df1['Pstng.Date']/100000000\n",
    "    \n",
    "    \n",
    "    print('Starting Prediction ....')\n",
    "    prediction=clf.predict(df1)\n",
    "    print('Generating Probabilities.....')\n",
    "    predictprob=clf.predict_proba(df1)\n",
    "    print('Finished prediction...')\n",
    "    print()\n",
    "    #df['Probability']=np.zeros(len(df))\n",
    "    print(df1.head())\n",
    "    \n",
    "\n",
    "    arr=['No Delay','Low Delay','High Delay']\n",
    "    arr1=['Low delay','Low Medium Delay','High Delay']#,'High Delay']#,'30 to 180','>180']#,'8-15 days','16-30 days','31-60 days','61-90 days','90-180 days','>180 days']\n",
    "    for i in range(len(arr)):\n",
    "        df[arr1[i]]=np.empty(len(df))\n",
    "    print('Computing Prediction Table............')\n",
    "    #for i in range(len(prediction)):\n",
    "        #flag=0\n",
    "        #if ( i % 500 == 0):\n",
    "         #   print('----------------------------------------------------------------------------------------------------------- i =',i)\n",
    "        #for k in range(len(redzone)):\n",
    "           # if df['Account'][i] == redzone[k] :\n",
    "             #   df['Prediction'][i]='>=10'\n",
    "              #  flag=1\n",
    "              #  print('----------------------------------------------------------------RED ALERT------------------------------------------ ')\n",
    "                \n",
    "        #if(flag == 0):\n",
    "    \n",
    "        #print(arr[prediction[i]])\n",
    "        #arr2=\"\"\n",
    "        #print('Actual:',df['Arr..Clearing...Net.Due.Date.'][i])\n",
    "        #print('--------------------------------------------------------')\n",
    "    df['Prediction']=prediction\n",
    "    #df['Prediction']=np.where(df['Prediction'] == 0, 'No Delay',(np.where(df['Prediction'] == 1, 'Low Delay',(np.where(df['Prediction'] == 2, 'High Delay','')))))\n",
    "    for i in range(len(arr1)):\n",
    "        df[arr1[i]]=np.around(predictprob[:,i],decimals=2)\n",
    "        \n",
    "    print(df.head(5))\n",
    "    #df['Payment Within Next Few Days']=np.where(df['Prediction'] == 1, 'Yes',(np.where((df['Low delay']>0.2) & (df['High Delay']<0.4)),'Yes','No'))\n",
    "    print()\n",
    "    print(df.head())\n",
    "    print('Writing to csv file')\n",
    "    print('-----------------------------------------------------------------------------------------')\n",
    "    df.to_csv('Predictedout6.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Finished Reading...\n",
      "\n",
      "   BusA CCAr     Reference  Account          Customer.Name   Zone  Bran  Ty  \\\n",
      "0  9600  BPR  1.115502e+09   928860  TATA STEEL PROCESSING  North  GBAD  RV   \n",
      "1  9600  BPR  1.115502e+09   928860  TATA STEEL PROCESSING  North  GBAD  RV   \n",
      "2  2500  BPR  2.410320e+09   928860  TATA STEEL PROCESSING  North  GBAD  RV   \n",
      "3  1000  BPR  2.412144e+09   928860  TATA STEEL PROCESSING  North  GBAD  RV   \n",
      "4  1000  BPR  2.412161e+09   928860  TATA STEEL PROCESSING  North  GBAD  RV   \n",
      "\n",
      "    Status  Local.Crcy.Amt  ...  Doc.Chq.dt Month Pstng.Date  Net.Due.Dt  \\\n",
      "0  Overdue            0.04  ...  31.10.2018   Oct   31-10-18    10-11-18   \n",
      "1   Not OD            0.12  ...  05.11.2018   Nov   05-11-18    15-11-18   \n",
      "2  Overdue            0.16  ...  03.11.2018   Nov   03-11-18    13-11-18   \n",
      "3  Overdue            0.14  ...  31.10.2018   Oct   31-10-18    10-11-18   \n",
      "4   Not OD            0.14  ...  16.12.2018   Dec   16-12-18    26-12-18   \n",
      "\n",
      "       G.L  Year Clrng.doc.  Clearing  Pending_Amy  pending_cnt  \n",
      "0  1221001  2019   93892774  15-11-18         0.60            4  \n",
      "1  1221001  2019   93892774  15-11-18         1.42           10  \n",
      "2  1221001  2019   93892774  15-11-18         1.26            9  \n",
      "3  1221001  2019   93892774  15-11-18         0.60            4  \n",
      "4  1221001  2019   94154498  26-12-18         0.00            0  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "\n",
      "Preprocessing started .......\n",
      "Y s in the initial set\n",
      "(array([0, 1, 2]), array([70220, 12017, 11414], dtype=int64))\n",
      "----------------------------------------------------------------------------------------\n",
      "Y s in the set\n",
      "(array([0, 1, 2]), array([70220, 71167, 70448], dtype=int64))\n",
      "----------------------------------------------------------------------------------------\n",
      "   BusA  CCAr   Account  Zone  Bran  Local.Crcy.Amt  PayT  Month  Pstng.Date  \\\n",
      "0   6.0   0.0  928860.0   1.0   9.0            0.04  10.0    6.0  20181031.0   \n",
      "1   6.0   0.0  928860.0   1.0   9.0            0.12  10.0    5.0  20181105.0   \n",
      "2   2.0   0.0  928860.0   1.0   9.0            0.16  10.0    5.0  20181103.0   \n",
      "3   0.0   0.0  928860.0   1.0   9.0            0.14  10.0    6.0  20181031.0   \n",
      "4   6.0   0.0  928860.0   1.0   9.0            0.16   5.0    5.0  20181105.0   \n",
      "\n",
      "   Net.Due.Dt  Pending_Amy  pending_cnt  \n",
      "0  20181110.0         0.60          4.0  \n",
      "1  20181115.0         1.42         10.0  \n",
      "2  20181113.0         1.26          9.0  \n",
      "3  20181110.0         0.60          4.0  \n",
      "4  20181110.0         1.42         10.0  \n",
      "Preprocessing Done...........\n",
      "--------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "trainpath='My Data_Delay.csv'\n",
    "\n",
    "print('Reading data...')\n",
    "df1=pd.read_csv(trainpath)\n",
    "print('Finished Reading...')\n",
    "print()\n",
    "print(df1.head(5))\n",
    "print()\n",
    "date=20181201\n",
    "df3=copy.deepcopy(df1)\n",
    "print('Preprocessing started .......')\n",
    "df2,df4,y,busa,ccar,month,zone,bran,payt=preprocessor(df1,date)\n",
    "print(df2.head(5))\n",
    "print('Preprocessing Done...........')\n",
    "print('--------------------------------------------------------------------------------------------------------')\n",
    "#print('RED ZONE VISUALISATION....')\n",
    "\n",
    "#visualisation_red(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.drop(columns=['Status','Clrng.doc.'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Starting............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[1 1 0 ... 2 0 2].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-5fb0f9018f94>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mt1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training Starting............'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training Done...................'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Time Taken'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-1c9067815c47>\u001b[0m in \u001b[0;36mtrainer\u001b[1;34m(df1, y)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0my_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[0my_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mytest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    516\u001b[0m                 self._categorical_features, copy=True)\n\u001b[0;32m    517\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 518\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_legacy_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    427\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, handle_unknown)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_X\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\u001b[0m in \u001b[0;36m_check_X\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \"\"\"\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mX_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'dtype'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_temp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    550\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[1 1 0 ... 2 0 2].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1=time.time()\n",
    "print('Training Starting............')\n",
    "clf=trainer(df2,y)\n",
    "print('Training Done...................')\n",
    "print('Time Taken',time.time()-t1)\n",
    "print('-----------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances\n",
      "('BusA', 0.05102617907456457)\n",
      "('CCAr', 0.0)\n",
      "('Account', 0.1549349880830567)\n",
      "('Zone', 0.03731605800307801)\n",
      "('Bran', 0.0860613122837913)\n",
      "('Local.Crcy.Amt', 0.10598695992884784)\n",
      "('PayT', 0.06240314414088058)\n",
      "('Month', 0.06473480204750053)\n",
      "('Pstng.Date', 0.09680186211229673)\n",
      "('Net.Due.Dt', 0.17937572838534319)\n",
      "('Pending_Amy', 0.08722421231181372)\n",
      "('pending_cnt', 0.07413475362882688)\n"
     ]
    }
   ],
   "source": [
    "print('Feature Importances')\n",
    "for feature in zip(df2.columns, clf.feature_importances_):\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1479, 22)\n",
      "(1479, 22)\n",
      "Starting Prediction ....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  35 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 156 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 300 out of 300 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  35 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 156 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Probabilities.....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done 300 out of 300 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished prediction...\n",
      "\n",
      "   BusA  CCAr  Account  Zone  Bran  Local.Crcy.Amt  PayT  Month  Pstng.Date  \\\n",
      "0     0     0   928860     1     9            0.14    10      2    20181216   \n",
      "1     0     0   928860     1     9            0.14    10      2    20181217   \n",
      "2     6     0   927881     2    21            0.18    10      2    20181214   \n",
      "3     6     0   927881     2    21            0.18     5      2    20181214   \n",
      "4     6     0   927881     2    21            0.18     5      2    20181214   \n",
      "\n",
      "   Net.Due.Dt  Pending_Amy  pending_cnt  \n",
      "0    20181226         0.00            0  \n",
      "1    20181227         0.14            1  \n",
      "2    20181224         2.16           12  \n",
      "3    20181219         2.16           12  \n",
      "4    20181219         2.16           12  \n",
      "Computing Prediction Table............\n",
      "   BusA CCAr     Reference  Account          Customer.Name   Zone  Bran  Ty  \\\n",
      "0  1000  BPR  2.412161e+09   928860  TATA STEEL PROCESSING  North  GBAD  RV   \n",
      "1  1000  BPR  2.412161e+09   928860  TATA STEEL PROCESSING  North  GBAD  RV   \n",
      "2  9600  BPR  2.147091e+09   927881   SAMRAT IRONS PVT LTD  South  SBAD  RV   \n",
      "3  9600  BPR  2.147091e+09   927881   SAMRAT IRONS PVT LTD  South  SBAD  RV   \n",
      "4  9600  BPR  2.147091e+09   927881   SAMRAT IRONS PVT LTD  South  SBAD  RV   \n",
      "\n",
      "   Local.Crcy.Amt  Arr..Clearing...Net.Due.Date.  ... Net.Due.Dt      G.L  \\\n",
      "0            0.14                              0  ...   20181226  1221001   \n",
      "1            0.14                             -1  ...   20181227  1221001   \n",
      "2            0.18                              5  ...   20181224  1221001   \n",
      "3            0.18                              2  ...   20181219  1221001   \n",
      "4            0.18                              2  ...   20181219  1221001   \n",
      "\n",
      "   Year  Clearing Pending_Amy  pending_cnt  Prediction  Low delay  \\\n",
      "0  2019  20181226        0.00            0           2       0.23   \n",
      "1  2019  20181226        0.14            1           0       1.00   \n",
      "2  2019  20181229        2.16           12           0       0.86   \n",
      "3  2019  20181221        2.16           12           1       0.15   \n",
      "4  2019  20181221        2.16           12           1       0.15   \n",
      "\n",
      "   Low Medium Delay  High Delay  \n",
      "0              0.08        0.69  \n",
      "1              0.00        0.00  \n",
      "2              0.14        0.00  \n",
      "3              0.83        0.01  \n",
      "4              0.83        0.01  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "\n",
      "   BusA CCAr     Reference  Account          Customer.Name   Zone  Bran  Ty  \\\n",
      "0  1000  BPR  2.412161e+09   928860  TATA STEEL PROCESSING  North  GBAD  RV   \n",
      "1  1000  BPR  2.412161e+09   928860  TATA STEEL PROCESSING  North  GBAD  RV   \n",
      "2  9600  BPR  2.147091e+09   927881   SAMRAT IRONS PVT LTD  South  SBAD  RV   \n",
      "3  9600  BPR  2.147091e+09   927881   SAMRAT IRONS PVT LTD  South  SBAD  RV   \n",
      "4  9600  BPR  2.147091e+09   927881   SAMRAT IRONS PVT LTD  South  SBAD  RV   \n",
      "\n",
      "   Local.Crcy.Amt  Arr..Clearing...Net.Due.Date.  ... Net.Due.Dt      G.L  \\\n",
      "0            0.14                              0  ...   20181226  1221001   \n",
      "1            0.14                             -1  ...   20181227  1221001   \n",
      "2            0.18                              5  ...   20181224  1221001   \n",
      "3            0.18                              2  ...   20181219  1221001   \n",
      "4            0.18                              2  ...   20181219  1221001   \n",
      "\n",
      "   Year  Clearing Pending_Amy  pending_cnt  Prediction  Low delay  \\\n",
      "0  2019  20181226        0.00            0           2       0.23   \n",
      "1  2019  20181226        0.14            1           0       1.00   \n",
      "2  2019  20181229        2.16           12           0       0.86   \n",
      "3  2019  20181221        2.16           12           1       0.15   \n",
      "4  2019  20181221        2.16           12           1       0.15   \n",
      "\n",
      "   Low Medium Delay  High Delay  \n",
      "0              0.08        0.69  \n",
      "1              0.00        0.00  \n",
      "2              0.14        0.00  \n",
      "3              0.83        0.01  \n",
      "4              0.83        0.01  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "Writing to csv file\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "______________________________________  END  ____________________________________\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testpath= ''\n",
    "#test=pd.read_csv(testpath)\n",
    "predict(df4,clf,busa,ccar,month,zone,bran,payt)\n",
    "print() \n",
    "print('______________________________________  END  ____________________________________')\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1455, 1459], dtype=int64),)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(df4['PayT']=='Z020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.drop([1456],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6], dtype=int64)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "busa.transform([9600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jt -t chesterish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
